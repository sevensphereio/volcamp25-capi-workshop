# Module 02: Networking avec Calico

**Dur√©e:** 15 minutes

---

## üéØ Objectifs & Concepts

### Ce que vous allez apprendre
- ‚úÖ Pourquoi les nodes sont NotReady sans CNI (Container Network Interface)
- ‚úÖ Comment ClusterResourceSet automatise le d√©ploiement d'addons
- ‚úÖ D√©ployer Calico automatiquement avec le pattern label-based
- ‚úÖ Passer les nodes de NotReady √† Ready

### Concepts cl√©s
**CNI (Container Network Interface):** Plugin r√©seau qui permet la communication pod-to-pod. Sans CNI, kubelet d√©clare les nodes NotReady car il ne peut pas garantir la connectivit√© r√©seau.

**ClusterResourceSet (CRS):** M√©canisme ClusterAPI pour d√©ployer automatiquement des ressources (addons) sur les workload clusters via s√©lection par labels. √âquivalent d'un "syst√®me d'installation automatique" : un label sur le cluster d√©clenche le d√©ploiement.

**Workflow CRS:**
```
1. Cr√©er ClusterResourceSet + ConfigMap (contient manifest)
2. labeliser le cluster cible
3. CRS controller d√©tecte le match et applique automatiquement
```

**Avantages vs installation manuelle:**
- Automatique une fois le labelling fait (pas de kubectl apply manuel)
- D√©claratif et versionnable Git (GitOps ready)
- R√©utilisable pour N clusters (m√™me label = m√™me addon)
- Self-service (nouveau cluster avec le bon label = addon auto-install√©)

---

## üìã Actions Pas-√†-Pas

### Action 1: Diagnostiquer le probl√®me r√©seau

**Objectif:** Comprendre pourquoi les nodes sont NotReady

**Commande:**
```bash
cd ~/02-networking-calico
kubectl --kubeconfig ~/01-premier-cluster/dev-cluster.kubeconfig get nodes
```

**Explication de la commande:**
- `--kubeconfig`: pointe vers le kubeconfig du workload cluster dev-cluster
- `get nodes`: affiche l'√©tat des nodes du cluster

**R√©sultat attendu:**
```
NAME                              STATUS     ROLES           AGE   VERSION
dev-cluster-control-plane-xxxx    NotReady   control-plane   5m    v1.32.8
dev-cluster-md-0-yyyyy-zzzzz      NotReady   <none>          4m    v1.32.8
dev-cluster-md-0-yyyyy-aaaaa      NotReady   <none>          4m    v1.32.8
```

**‚úÖ V√©rification:** Tous les nodes sont en STATUS NotReady. C'est normal √† ce stade : aucun CNI n'est install√©.

---

### Action 2: Identifier la cause (CNI manquant)

**Objectif:** Confirmer que le probl√®me vient de l'absence de CNI

**Commande:**
```bash
kubectl --kubeconfig ~/01-premier-cluster/dev-cluster.kubeconfig describe node dev-cluster-control-plane-* | grep -A 5 "Conditions:"
```

**Explication de la commande:**
- `describe node`: affiche les d√©tails d'un node
- `dev-cluster-control-plane-*`: wildcard pour matcher le nom du node control plane
- `grep -A 5 "Conditions:"`: filtre pour afficher les conditions du node (5 lignes apr√®s)

**R√©sultat attendu:**
```
Conditions:
  Type             Status
  Ready            False
  ...
  Message: network plugin is not ready: cni config uninitialized
```

**‚úÖ V√©rification:** Le message confirme "network plugin is not ready". Le CNI n'est pas configur√©.

---

### Action 3: Analyser le manifeste ClusterResourceSet

**Objectif:** Comprendre la structure du CRS avant de l'appliquer

**Commande:**
```bash
cat calico-crs.yaml | head -30
```

**Explication de la commande:**
- `cat`: affiche le contenu du fichier
- `head -30`: limite l'affichage aux 30 premi√®res lignes pour voir la structure

**R√©sultat attendu:**
```yaml
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  name: calico-cni
spec:
  clusterSelector:
    matchLabels:
      cni: calico          # Cible les clusters avec ce label
  resources:
  - name: calico-addon     # R√©f√©rence au ConfigMap
    kind: ConfigMap
  strategy: ApplyOnce      # Appliqu√© une seule fois
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: calico-addon
data:
  calico.yaml: |           # Manifeste Calico complet (7000+ lignes)
    ...
```

**‚úÖ V√©rification:** Le CRS contient 2 objets : ClusterResourceSet (r√®gle) + ConfigMap (manifeste Calico)

---

### Action 4: Cr√©er le ClusterResourceSet

**Objectif:** D√©ployer la r√®gle CRS dans le management cluster

**Commande:**
```bash
kubectl apply -f calico-crs.yaml -f calico-cm-crs.yaml
```

**Explication de la commande:**
- `apply -f`: cr√©e ou met √† jour les ressources d√©finies dans le fichier YAML
- `calico-crs.yaml`: fichier contenant ClusterResourceSet + ConfigMap

**R√©sultat attendu:**
```
clusterresourceset.addons.cluster.x-k8s.io/calico-cni created
configmap/calico-addon created
```

**‚úÖ V√©rification:** 2 objets cr√©√©s : le CRS (r√®gle) et le ConfigMap (donn√©es Calico)

---

### Action 5: V√©rifier le CRS cr√©√©

**Objectif:** Confirmer que le CRS existe et est actif

**Commande:**
```bash
kubectl get clusterresourceset
```

**Explication de la commande:**
- `get clusterresourceset`: liste tous les ClusterResourceSets du management cluster

**R√©sultat attendu:**
```
NAME         AGE
calico-cni   10s
```

**‚úÖ V√©rification:** Le CRS calico-cni appara√Æt dans la liste

---

### Action 6: Activer le CRS en labellant le cluster

**Objectif:** D√©clencher le d√©ploiement automatique de Calico

**Commande:**
```bash
kubectl label cluster dev-cluster cni=calico
```

**Explication de la commande:**
- `label cluster dev-cluster`: ajoute un label au cluster dev-cluster
- `cni=calico`: label qui matche le clusterSelector du CRS

**R√©sultat attendu:**
```
cluster.cluster.x-k8s.io/dev-cluster labeled
```

**‚úÖ V√©rification:** Le label est ajout√©. Le CRS controller va d√©tecter le match et appliquer Calico automatiquement dans les secondes qui suivent.

---

### Action 7: V√©rifier le label appliqu√©

**Objectif:** Confirmer que le label est bien pr√©sent sur le cluster

**Commande:**
```bash
kubectl get cluster dev-cluster --show-labels
```

**Explication de la commande:**
- `--show-labels`: affiche tous les labels du cluster dans la sortie

**R√©sultat attendu:**
```
NAME          PHASE        AGE   LABELS
dev-cluster   Provisioned  10m   cni=calico,environment=demo
```

**‚úÖ V√©rification:** Le label `cni=calico` est pr√©sent dans LABELS

---

### Action 8: Observer l'installation automatique de Calico

**Objectif:** Voir en temps r√©el l'apparition des pods Calico dans le workload cluster

**Commande:**
```bash
watch -n 2 'kubectl --kubeconfig ~/01-premier-cluster/dev-cluster.kubeconfig get pods -n kube-system'
```

**Explication de la commande:**
- `watch -n 2`: ex√©cute la commande toutes les 2 secondes et affiche le r√©sultat
- `-n kube-system`: limite l'affichage au namespace syst√®me o√π Calico se d√©ploie

**R√©sultat attendu (progression):**

**Minute 1:**
```
NAME                                    READY   STATUS              RESTARTS
calico-kube-controllers-xxx             0/1     ContainerCreating   0
calico-node-aaaa                        0/1     Init:0/3            0
calico-node-bbbb                        0/1     Init:0/3            0
coredns-xxx                             0/1     Pending             0
```

**Minute 2:**
```
NAME                                    READY   STATUS    RESTARTS
calico-kube-controllers-xxx             1/1     Running   0
calico-node-aaaa                        1/1     Running   0
calico-node-bbbb                        1/1     Running   0
coredns-xxx                             1/1     Running   0
```

**‚úÖ V√©rification:** Tous les pods Calico (calico-node DaemonSet + calico-kube-controllers) sont Running. CoreDNS passe aussi √† Running car il peut maintenant obtenir une IP r√©seau. Appuyez sur Ctrl+C pour arr√™ter.

---

### Action 9: Observer les nodes passer √† Ready

**Objectif:** Confirmer que les nodes d√©tectent le CNI et passent √† Ready

**Commande:**
```bash
watch -n 2 'kubectl --kubeconfig ~/01-premier-cluster/dev-cluster.kubeconfig get nodes'
```

**Explication de la commande:**
- `watch -n 2`: rafra√Æchit l'affichage toutes les 2 secondes
- `get nodes`: affiche l'√©tat des nodes

**R√©sultat attendu (progression):**

**Avant (~1 minute):**
```
NAME                              STATUS   ROLES           AGE
dev-cluster-control-plane-xxxx    Ready    control-plane   11m
dev-cluster-md-0-yyyyy-zzzzz      Ready    <none>          10m
dev-cluster-md-0-yyyyy-aaaaa      Ready    <none>          10m
```

**‚úÖ V√©rification:** 3/3 nodes sont Ready. Le CNI est fonctionnel. Appuyez sur Ctrl+C.

---

### Action 10: Tester la communication r√©seau

**Objectif:** Valider que les pods peuvent obtenir des IPs et communiquer

**Commande:**
```bash
kubectl --kubeconfig ~/01-premier-cluster/dev-cluster.kubeconfig run test-pod --image=nginx --restart=Never
kubectl --kubeconfig ~/01-premier-cluster/dev-cluster.kubeconfig get pod test-pod -o wide
```

**Explication de la commande:**
- `run test-pod`: cr√©e un pod simple avec nginx
- `--restart=Never`: cr√©e un pod simple (pas un Deployment)
- `get pod -o wide`: affiche les d√©tails incluant l'IP assign√©e

**R√©sultat attendu:**
```
NAME       READY   STATUS    RESTARTS   AGE   IP              NODE
test-pod   1/1     Running   0          20s   192.168.X.Y     dev-cluster-md-0-...
```

**‚úÖ V√©rification:** Le pod a une IP du range 192.168.0.0/16 (d√©fini dans dev-cluster.yaml). Le r√©seau fonctionne.

**Cleanup:**
```bash
kubectl --kubeconfig ~/01-premier-cluster/dev-cluster.kubeconfig delete pod test-pod
```

---

### Action 11: Validation automatique du module

**Objectif:** V√©rifier que toutes les √©tapes sont r√©ussies

**Commande:**
```bash
./validation.sh
```

**Explication de la commande:**
- Script qui v√©rifie : CRS existe, label appliqu√©, pods Calico Running, nodes Ready

**R√©sultat attendu:**
```
üîç Module 02: Validation Networking Calico
===========================================

‚úÖ ClusterResourceSet calico-cni existe
‚úÖ ConfigMap calico-addon existe
‚úÖ Cluster dev-cluster a le label cni=calico
‚úÖ CRS appliqu√© sur le cluster
‚úÖ Calico pods Running (4/4)
‚úÖ 3/3 nodes Ready
‚úÖ CoreDNS pods Running (2/2)

===========================================
üéâ Module 02 termin√© avec succ√®s!
üöÄ Pr√™t pour Module 03: k0smotron Control Planes
===========================================
```

**‚úÖ V√©rification:** Tous les checks passent. Le r√©seau est fonctionnel.

---

## üí° Comprendre en Profondeur

### Pourquoi CoreDNS √©tait Pending avant Calico ?

CoreDNS est un pod qui n√©cessite une IP r√©seau pour fonctionner. Sans CNI :
- Le scheduler ne peut pas assigner d'IP au pod
- Les routes r√©seau n'existent pas
- Le pod reste en Pending

D√®s que Calico est install√© :
- Le CNI assigne une IP du range configur√©
- Les routes sont cr√©√©es automatiquement
- CoreDNS peut d√©marrer et fournir le DNS au cluster

**Ordre critique:** CNI AVANT tout autre addon r√©seau.

---

### ClusterResourceSet : ApplyOnce vs Reconcile

Deux strat√©gies d'application :

**ApplyOnce (utilis√© ici):**
- Appliqu√© une seule fois au moment du match
- Modifications ult√©rieures du CRS ne sont pas propag√©es
- Convient pour addons g√©r√©s ind√©pendamment apr√®s installation

**Reconcile:**
- R√©appliqu√© r√©guli√®rement pour forcer la configuration
- Modifications du CRS propag√©es automatiquement
- Convient pour garantir la conformit√© continue

---

### Pattern Label-Based : Flexibilit√© GitOps

Le s√©lecteur par labels permet des strat√©gies flexibles :

```yaml
# Exemple : tous les clusters production ET Europe
clusterSelector:
  matchLabels:
    environment: production
    region: europe
```

**Avantages :**
- Self-service : √©quipes dev ajoutent un label = addon d√©ploy√©
- Gouvernance : √©quipes platform contr√¥lent les CRS
- √âvolutivit√© : 1 CRS pour 100+ clusters

---

### Calico : Plus qu'un CNI

Calico offre √©galement :
- **Network Policies :** Firewall pod-to-pod (s√©curit√©)
- **BGP routing :** Routage avanc√© pour on-premise
- **Observability :** M√©triques r√©seau d√©taill√©es

---

## üîç Troubleshooting

**Pods Calico ne d√©marrent pas :**
```bash
kubectl --kubeconfig ~/01-premier-cluster/dev-cluster.kubeconfig get events -n kube-system --sort-by='.lastTimestamp'
kubectl --kubeconfig ~/01-premier-cluster/dev-cluster.kubeconfig logs -n kube-system calico-node-xxx
```

**CRS ne s'applique pas :**
```bash
# V√©rifier le label
kubectl get cluster dev-cluster --show-labels

# Logs du CRS controller
kubectl logs -n capi-system deployment/capi-controller-manager | grep clusterresourceset
```

**Nodes restent NotReady :**
```bash
# Attendre 1-2 minutes apr√®s installation Calico
# V√©rifier que tous les pods Calico sont Running
kubectl --kubeconfig ~/01-premier-cluster/dev-cluster.kubeconfig get pods -n kube-system -l k8s-app=calico-node
```

---

## ‚è≠Ô∏è Prochaine √âtape

**Module 03 (15 min):** k0smotron Control Planes Virtuels
- Comprendre les √©conomies de ressources (55%)
- Cr√©er un cluster k0smotron
- Comparer avec Docker provider

```bash
cd ~/03-k0smotron
cat commands.md
```
