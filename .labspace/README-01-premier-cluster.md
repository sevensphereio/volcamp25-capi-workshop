# Module 01: Premier Cluster ClusterAPI

**Dur√©e:** 15 minutes | **Objectif:** Cr√©er votre premier cluster Kubernetes avec ClusterAPI Docker provider

---

## üéØ Ce que vous allez faire

Dans ce module, vous allez :
1. **G√©n√©rer** un manifeste de cluster avec `clusterctl generate`
2. **Cr√©er** un cluster Kubernetes d√©clarativement
3. **Observer** la cr√©ation automatique du control plane et des workers
4. **Comprendre** l'architecture √† 7 objets interconnect√©s
5. **Acc√©der** au cluster cr√©√©

---

## üìÅ Fichiers du Module

- **[commands.md](commands.md)** - Instructions d√©taill√©es avec th√©orie et pratique
- **[QUICKSTART.md](QUICKSTART.md)** - Guide rapide (commandes uniquement)
- **[create-cluster.sh](create-cluster.sh)** - üÜï Script de cr√©ation automatique complet
- **[validation.sh](validation.sh)** - Script de validation automatique
- **dev-cluster.yaml** - Manifeste g√©n√©r√© (cr√©√© durant le module)
- **dev-cluster.kubeconfig** - Fichier kubeconfig du cluster (cr√©√© durant le module)

---

## ‚ö° Quick Start

### Option 1 : Script Automatique (Recommand√©)

```bash
cd /home/volcampdev/workshop-express/01-premier-cluster
./create-cluster.sh
```

Le script `create-cluster.sh` automatise toutes les √©tapes :
- G√©n√®re le manifeste avec `clusterctl generate`
- Cr√©e le cluster
- Attend que le cluster soit provisionn√©
- R√©cup√®re le kubeconfig
- Ex√©cute la validation

### Option 2 : Commandes Manuelles

```bash
cd /home/volcampdev/workshop-express/01-premier-cluster

# G√©n√©rer le manifeste
clusterctl generate cluster dev-cluster \
  --flavor development \
  --kubernetes-version v1.32.8 \
  --control-plane-machine-count=1 \
  --worker-machine-count=2 \
  > dev-cluster.yaml

# Cr√©er le cluster
kubectl apply -f dev-cluster.yaml

# Observer la cr√©ation (Ctrl+C pour arr√™ter)
watch -n 2 'kubectl get clusters,machines'

# Valider
./validation.sh
```

---

## üèóÔ∏è Architecture : 1 Manifeste = 7 Objets

Le fichier `dev-cluster.yaml` g√©n√©r√© contient **7 objets ClusterAPI** interconnect√©s :

```
1. Cluster                    ‚Üí Chef d'orchestre (coordonne tout)
2. DockerCluster              ‚Üí Infrastructure (r√©seau, load balancer)
3. KubeadmControlPlane        ‚Üí D√©finition du control plane
4. DockerMachineTemplate (CP) ‚Üí Template pour cr√©er les CP nodes
5. MachineDeployment          ‚Üí D√©finition des workers (scalable!)
6. DockerMachineTemplate (W)  ‚Üí Template pour cr√©er les workers
7. KubeadmConfigTemplate      ‚Üí Configuration bootstrap des workers
```

**Pourquoi 7 objets ?** S√©paration des responsabilit√©s. Chaque objet a un r√¥le pr√©cis, permettant :
- **Modularit√©** : Changer la version K8s = modifier 1 objet
- **R√©utilisabilit√©** : M√™me template pour plusieurs d√©ploiements
- **Portabilit√©** : Changer de provider = remplacer 2 objets sur 7

---

## üöÄ Configuration du Cluster

Le cluster g√©n√©r√© avec `clusterctl generate` a les caract√©ristiques suivantes :

| Param√®tre | Valeur | Justification |
|-----------|--------|---------------|
| **Flavor** | `development` | Optimis√© pour dev local (ressources minimales) |
| **Version K8s** | `v1.32.8` | Version stable r√©cente |
| **Control Plane** | 1 node | Suffisant pour dev/test |
| **Workers** | 2 nodes | Permet de tester la distribution de pods |
| **Provider** | Docker (CAPD) | Rapide, local, sans co√ªt |

**En production :**
- Control plane: 3 nodes (HA)
- Workers: 3+ nodes (selon charge)
- Provider: AWS/Azure/GCP

---

## üîÑ Workflow de Cr√©ation

Apr√®s `kubectl apply -f dev-cluster.yaml` :

```
T+0s   : kubectl envoie les 7 objets √† l'API server
T+1s   : ClusterAPI controller d√©tecte le nouveau Cluster
T+2s   : DockerCluster controller cr√©e le load balancer
T+5s   : KubeadmControlPlane cr√©e la premi√®re Machine pour le CP
T+10s  : Docker provider cr√©e un container pour le CP
T+30s  : Kubeadm bootstrap installe Kubernetes dans le container
T+60s  : Control plane UP! API server accessible
T+65s  : MachineDeployment cr√©e 2 Machines workers
T+70s  : Docker provider cr√©e 2 containers pour les workers
T+120s : Workers joignent le control plane
T+180s : üéâ Cluster Provisioned! (nodes NotReady - pas de CNI)
```

**Dur√©e totale :** ~3 minutes (containers vs 5-8min avec VMs cloud)

---

## ‚úÖ Validation

```bash
./validation.sh
```

**R√©sultat attendu :**
```
‚úÖ Cluster dev-cluster existe
‚úÖ Cluster phase = Provisioned
‚úÖ Control plane ready (1/1)
‚úÖ 3 Machines en phase Running
‚úÖ Kubeconfig r√©cup√©rable
‚úÖ 3 nodes visibles dans le workload cluster
‚ö†Ô∏è  Nodes NotReady (normal - CNI manquant)
```

---

## ‚ö†Ô∏è √âtat NotReady : C'est Normal !

Les nodes sont en √©tat `NotReady` car :
- **Aucun CNI install√©** : Pas de plugin r√©seau
- **Pas de communication pod-to-pod** : Les pods ne peuvent pas se parler
- **CoreDNS bloqu√©** : Attend le r√©seau pour d√©marrer

**Solution :** Le **Module 02** installera Calico CNI via ClusterResourceSet automatiquement !

---

## üìä Commandes Utiles

### Observer les ressources
```bash
# Voir les clusters
kubectl get clusters

# Voir les machines (CP + workers)
kubectl get machines -o wide

# Voir le control plane
kubectl get kubeadmcontrolplane

# Voir le d√©ploiement de workers
kubectl get machinedeployment

# Voir les containers Docker
docker ps | grep dev-cluster
```

### Acc√©der au workload cluster
```bash
# R√©cup√©rer le kubeconfig
clusterctl get kubeconfig dev-cluster > dev-cluster.kubeconfig

# Voir les nodes
kubectl --kubeconfig dev-cluster.kubeconfig get nodes

# Voir les pods (CoreDNS sera Pending)
kubectl --kubeconfig dev-cluster.kubeconfig get pods -A
```

### Scaling (apr√®s cr√©ation)
```bash
# Scaler les workers de 2 √† 5
kubectl scale machinedeployment dev-cluster-md-0 --replicas=5

# Observer le scaling
watch -n 2 'kubectl get machines'
```

---

## üîß D√©pannage

### Cluster reste en Pending

**Diagnostic :**
```bash
kubectl describe cluster dev-cluster
kubectl logs -n capi-system deployment/capi-controller-manager -f
kubectl logs -n capd-system deployment/capd-controller-manager -f
```

**Causes fr√©quentes :**
- Docker daemon inaccessible
- Ressources insuffisantes
- Port d√©j√† utilis√© (load balancer)

### Machine ne d√©marre pas

**Diagnostic :**
```bash
kubectl describe machine <machine-name>
docker ps -a | grep dev-cluster
docker logs <container-name>
```

### Kubeconfig inaccessible

**V√©rifier que le cluster est Provisioned :**
```bash
kubectl get cluster dev-cluster -o jsonpath='{.status.phase}'
# Doit afficher : Provisioned
```

---

## üéì Points Cl√©s √† Retenir

‚úÖ **clusterctl generate** : G√©n√®re des manifestes avec bonnes pratiques
‚úÖ **1 YAML = 7 objets** : S√©paration des responsabilit√©s
‚úÖ **Machine ‚â† Node** : Machine (CRD) cr√©e une infra qui devient un Node (K8s)
‚úÖ **MachineDeployment** : Comme un Deployment K8s mais pour nodes
‚úÖ **NotReady = Normal** : Le CNI sera install√© au Module 02

---

## ‚è≠Ô∏è Prochaine √âtape

Une fois la validation r√©ussie :

```bash
cd ../02-networking-calico
cat commands.md
```

**Module 02 (15 min) :** Installer Calico CNI automatiquement
- Comprendre ClusterResourceSets
- Automatiser le d√©ploiement de Calico
- Passer les nodes √† Ready

---

**Temps total :** ~15 minutes | **Difficult√© :** ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ
