# Module 00-setup: Installation du Cluster de Management

**DurÃ©e:** 15 minutes
**Objectif:** CrÃ©er le cluster de management kind et installer tous les composants ClusterAPI nÃ©cessaires

---

## ğŸ“‘ Table des MatiÃ¨res

- [ğŸ¯ Objectifs & Concepts](#-objectifs--concepts)
- [ğŸ“‹ Actions Pas-Ã -Pas](#-actions-pas-Ã -pas)
- [ğŸ’¡ Comprendre en Profondeur](#-comprendre-en-profondeur)

---

## ğŸ¯ Objectifs & Concepts

### Ce que vous allez apprendre

âœ… CrÃ©er un cluster de management kind configurÃ© pour ClusterAPI
âœ… Initialiser ClusterAPI avec le provider Docker (CAPD)
âœ… Comprendre l'architecture Management vs Workload clusters

### Le Principe : Management Cluster = Usine Ã  Clusters

**Analogie :** Le cluster de management est comme une **usine automobile**. L'usine elle-mÃªme ne transporte pas de passagers, mais elle fabrique des voitures (workload clusters) qui le font.

```
Management Cluster (kind)
â”œâ”€â”€ ClusterAPI Controllers     â†’ Chefs d'atelier (orchestrent la fabrication)
â””â”€â”€ Docker Provider            â†’ ChaÃ®ne d'assemblage Docker

Produit â†’ Workload Clusters
â”œâ”€â”€ dev-cluster (Docker)       â†’ Voiture de dÃ©veloppement
â”œâ”€â”€ k0s-demo-cluster           â†’ Voiture Ã©lectrique (plus Ã©conome, provider installÃ© plus tard)
â””â”€â”€ multi-clusters             â†’ Flotte de vÃ©hicules
```

**Pourquoi sÃ©parer Management et Workload ?**
- **SÃ©curitÃ©** : Le control plane de la fabrique est isolÃ© des applications
- **StabilitÃ©** : Un workload cluster crashÃ© n'affecte pas les autres
- **ScalabilitÃ©** : 1 management cluster peut gÃ©rer 100+ workload clusters
- **OpÃ©rations** : Upgrades, backups simplifiÃ©s (1 seul point de contrÃ´le)

---

### Les 2 Composants Essentiels

#### 1ï¸âƒ£ ClusterAPI Core (CAPI)

**RÃ´le :** Framework central pour la gestion dÃ©clarative de clusters Kubernetes

**Composants installÃ©s :**
- `capi-controller-manager` : Orchestrateur principal (Cluster, Machine CRDs)
- `capi-kubeadm-bootstrap-controller` : Bootstrap nodes avec kubeadm
- `capi-kubeadm-control-plane-controller` : Gestion control planes HA

**Version :** v1.11.1

---

#### 2ï¸âƒ£ Docker Provider (CAPD)

**RÃ´le :** Provider d'infrastructure pour crÃ©er des clusters locaux avec Docker

**Pourquoi Docker Provider ?**
- **Vitesse** : Clusters en 2-3 minutes (vs 8-10min avec cloud VMs)
- **CoÃ»t zÃ©ro** : Pas de facture AWS/Azure
- **IdÃ©al pour** : DÃ©veloppement, CI/CD, formation, testing

**Composant installÃ© :**
- `capd-controller-manager` : CrÃ©e containers Docker simulant des VMs

**âš ï¸ Production :** Remplacer par CAPA (AWS), CAPZ (Azure), CAPG (GCP)

**Note :** Les autres providers (k0smotron, Helm Addon) seront installÃ©s plus tard, dans les modules oÃ¹ ils sont utilisÃ©s.

---

## ğŸ“‹ Actions Pas-Ã -Pas

> **ğŸ’¡ Raccourci :** Pour un setup automatisÃ© complet, utilisez `./setup.sh` qui exÃ©cute toutes les Ã©tapes ci-dessous. Pour une comprÃ©hension dÃ©taillÃ©e, suivez les Ã©tapes manuelles.

### Ã‰tape 1 : VÃ©rifier que les outils sont installÃ©s

**Objectif :** Confirmer que Docker, kind, kubectl, clusterctl, helm sont disponibles

**Commande :**
```bash
cd /home/volcampdev/workshop-express/00-setup-management
cat > verify-tools.sh << 'EOF'
#!/bin/bash
echo "ğŸ” VÃ©rification des outils..."
for tool in docker kind kubectl clusterctl helm; do
  if command -v $tool &> /dev/null; then
    version=$($tool version 2>/dev/null | head -1 || echo "installÃ©")
    echo "âœ… $tool: $version"
  else
    echo "âŒ $tool: NON INSTALLÃ‰"
    exit 1
  fi
done
echo ""
echo "âœ… Tous les outils sont prÃªts!"
EOF
chmod +x verify-tools.sh
./verify-tools.sh
```

**Explication :**
- Script bash qui teste chaque outil requis
- `command -v` : VÃ©rifie si la commande existe dans PATH
- Affiche la version pour confirmation

**RÃ©sultat attendu :**
```
ğŸ” VÃ©rification des outils...
âœ… docker: Docker version 27.4.0
âœ… kind: kind v0.30.0
âœ… kubectl: Client Version: v1.32.0
âœ… clusterctl: v1.11.1
âœ… helm: v3.19.0

âœ… Tous les outils sont prÃªts!
```

**âŒ Si un outil manque :** Retourner au Module 00-introduction pour l'installer

---

### Ã‰tape 2 : CrÃ©er le cluster de management avec kind

**Objectif :** CrÃ©er un cluster Kubernetes local qui hÃ©bergera ClusterAPI

**Commande :**
```bash
cat > management-cluster-config.yaml << 'EOF'
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: capi-management
nodes:
  - role: control-plane
    extraMounts:
      - hostPath: /var/run/docker.sock
        containerPath: /var/run/docker.sock
    extraPortMappings:
      - containerPort: 30080
        hostPort: 30080
        protocol: TCP
    kubeadmConfigPatches:
      - |
        kind: InitConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "ingress-ready=true"
            authorization-mode: "Webhook"
EOF

kind create cluster --config management-cluster-config.yaml
```

**Explication de la configuration :**

**1. Nom du cluster :**
```yaml
name: capi-management
```
- Identifie le cluster (contexte kubectl sera `kind-capi-management`)

**2. Montage socket Docker :**
```yaml
extraMounts:
  - hostPath: /var/run/docker.sock
    containerPath: /var/run/docker.sock
```
- **ESSENTIEL** : Monte la socket Docker de l'hÃ´te dans le container kind
- Permet au Docker Provider (CAPD) de crÃ©er des containers pour les workload clusters
- Sans cela, CAPD ne peut pas communiquer avec le daemon Docker

**3. Port mapping 30080 :**
```yaml
extraPortMappings:
  - containerPort: 30080
    hostPort: 30080
```
- Expose le port 30080 du container kind vers localhost
- NÃ©cessaire pour tester les applications dÃ©ployÃ©es (nginx, etc.)
- Dans les modules suivants, vous accÃ©derez Ã  http://localhost:30080

**4. Labels du node :**
```yaml
node-labels: "ingress-ready=true"
```
- Marque le node comme prÃªt pour ingress controllers
- UtilisÃ© par les applications avec LoadBalancer/Ingress

**5. Authorization mode :**
```yaml
authorization-mode: "Webhook"
```
- Active les webhooks d'admission
- Requis par cert-manager et autres controllers avancÃ©s

**RÃ©sultat attendu :**
```
Creating cluster "capi-management" ...
 âœ“ Ensuring node image (kindest/node:v1.32.0)
 âœ“ Preparing nodes
 âœ“ Writing configuration
 âœ“ Starting control-plane
 âœ“ Installing CNI
 âœ“ Installing StorageClass
Set kubectl context to "kind-capi-management"
You can now use your cluster with:

kubectl cluster-info --context kind-capi-management
```

**âœ… VÃ©rification :**
```bash
kubectl cluster-info --context kind-capi-management
kubectl get nodes
```

Vous devriez voir :
```
NAME                            STATUS   ROLES           AGE   VERSION
capi-management-control-plane   Ready    control-plane   1m    v1.32.0
```

---

### Ã‰tape 3 : Initialiser ClusterAPI avec le Docker Provider

**Objectif :** Installer les controllers ClusterAPI dans le management cluster

**Commande :**
```bash
export CLUSTER_TOPOLOGY=true
export EXP_CLUSTER_RESOURCE_SET=true
clusterctl init --infrastructure docker
```

**Explication de la commande :**
- `export CLUSTER_TOPOLOGY=true` : Active la feature gate Cluster Topology (ClusterClass)
- `export EXP_CLUSTER_RESOURCE_SET=true` : Active la feature gate ClusterResourceSet (installation automatique d'addons)
- `clusterctl init` : Commande d'initialisation ClusterAPI
- `--infrastructure docker` : SpÃ©cifie le provider (CAPD)
- Installe automatiquement :
  - ClusterAPI Core
  - Kubeadm Bootstrap Provider
  - Kubeadm Control Plane Provider
  - Docker Infrastructure Provider
  - cert-manager (dÃ©pendance requise)

**RÃ©sultat attendu :**
```
Fetching providers
Installing cert-manager Version="v1.18.2"
Waiting for cert-manager to be available...
Installing Provider="cluster-api" Version="v1.11.1" TargetNamespace="capi-system"
Installing Provider="bootstrap-kubeadm" Version="v1.11.1" TargetNamespace="capi-kubeadm-bootstrap-system"
Installing Provider="control-plane-kubeadm" Version="v1.11.1" TargetNamespace="capi-kubeadm-control-plane-system"
Installing Provider="infrastructure-docker" Version="v1.11.1" TargetNamespace="capd-system"

Your management cluster has been initialized successfully!

You can now create your first workload cluster by running the following:

  clusterctl generate cluster [name] --infrastructure docker | kubectl apply -f -
```

**â³ Temps d'installation :** 1-2 minutes

**âœ… VÃ©rification :**
```bash
kubectl get pods -A | grep -E "(capi|cert-manager)"
```

**RÃ©sultat attendu :**
```
NAMESPACE                          NAME                                                            READY   STATUS
cert-manager                       cert-manager-xxx                                                1/1     Running
cert-manager                       cert-manager-cainjector-xxx                                     1/1     Running
cert-manager                       cert-manager-webhook-xxx                                        1/1     Running
capi-kubeadm-bootstrap-system      capi-kubeadm-bootstrap-controller-manager-xxx                   1/1     Running
capi-kubeadm-control-plane-system  capi-kubeadm-control-plane-controller-manager-xxx               1/1     Running
capi-system                        capi-controller-manager-xxx                                     1/1     Running
capd-system                        capd-controller-manager-xxx                                     1/1     Running
```

**ğŸ” 5 namespaces crÃ©Ã©s = 5 controllers** :
1. **cert-manager** : Gestion automatique certificats TLS
2. **capi-system** : Controller principal ClusterAPI
3. **capi-kubeadm-bootstrap-system** : Bootstrap nodes avec kubeadm
4. **capi-kubeadm-control-plane-system** : Gestion control planes
5. **capd-system** : Docker infrastructure provider

---

### Ã‰tape 4 : VÃ©rifier le montage de la socket Docker

**Objectif :** Confirmer que CAPD peut communiquer avec Docker pour crÃ©er des workload clusters

**Commande :**
```bash
./verify-docker-socket.sh
```

**RÃ©sultat attendu :**
```
ğŸ” VÃ©rification du montage de la socket Docker
===============================================

âœ… Cluster kind 'capi-management' existe

ğŸ”§ Test d'accÃ¨s Ã  la socket Docker depuis le cluster kind...

âœ… Socket Docker est montÃ©e et accessible: /var/run/docker.sock
   Permissions: srw-rw---- root docker

ğŸ³ Test de connectivitÃ© Docker depuis le cluster kind...

âœ… Communication avec Docker Daemon rÃ©ussie
   Containers visibles: 2

ğŸ›ï¸  VÃ©rification CAPD Controller...

âœ… Namespace capd-system existe
âœ… CAPD Controller est Running (1/1)

   VÃ©rification des logs CAPD pour erreurs Docker...
   âœ… Aucune erreur Docker dans les logs CAPD

===============================================
ğŸ‰ VÃ©rification terminÃ©e avec succÃ¨s!

ğŸ“Š RÃ©sumÃ©:
  âœ… Socket Docker montÃ©e: /var/run/docker.sock
  âœ… Communication Docker fonctionnelle
  âœ… CAPD peut crÃ©er des containers pour workload clusters

ğŸš€ Le cluster de management est prÃªt Ã  crÃ©er des workload clusters!
```

---

### Ã‰tape 5 : VÃ©rification finale complÃ¨te

**Objectif :** Confirmer que tous les composants sont opÃ©rationnels

**Commande :**
```bash
./validation.sh
```

**RÃ©sultat attendu :**
```
ğŸ” Module 00-setup: Validation Cluster de Management
====================================================

âœ… Cluster de management kind existe: capi-management
âœ… Contexte kubectl correctement configurÃ©: kind-capi-management
âœ… ClusterAPI Core installÃ© (capi-system)
âœ… Docker Provider installÃ© (capd-system)
âœ… cert-manager opÃ©rationnel
âœ… Tous les pods sont Running

ğŸ“Š RÃ©sumÃ© des Composants:
  âœ… ClusterAPI: v1.11.1
  âœ… Docker Provider: OpÃ©rationnel
  âœ… cert-manager: v1.18.2

====================================================
ğŸ‰ Module 00-setup terminÃ© avec succÃ¨s!
ğŸš€ Management cluster prÃªt Ã  crÃ©er des workload clusters
====================================================

Prochaine commande:
  cd ../01-premier-cluster
  cat commands.md
```

**âœ… Tous les tests passent :** Votre management cluster est prÃªt !

---

### Ã‰tape 6 : Explorer les ressources installÃ©es

**Objectif :** Comprendre ce qui a Ã©tÃ© installÃ©

**Commandes d'exploration :**

**6.1 - Voir tous les namespaces crÃ©Ã©s**
```bash
kubectl get namespaces | grep -E "(capi|cert-manager)"
```

**RÃ©sultat :**
```
capi-kubeadm-bootstrap-system      Active   10m
capi-kubeadm-control-plane-system  Active   10m
capi-system                        Active   10m
capd-system                        Active   10m
cert-manager                       Active   10m
```

---

**6.2 - Voir les CRDs (Custom Resource Definitions) installÃ©es**
```bash
kubectl get crds | grep cluster.x-k8s.io
```

**RÃ©sultat :**
```
clusters.cluster.x-k8s.io
dockerclusters.infrastructure.cluster.x-k8s.io
dockermachines.infrastructure.cluster.x-k8s.io
dockermachinetemplates.infrastructure.cluster.x-k8s.io
kubeadmconfigs.bootstrap.cluster.x-k8s.io
kubeadmconfigtemplates.bootstrap.cluster.x-k8s.io
kubeadmcontrolplanes.controlplane.cluster.x-k8s.io
machinedeployments.cluster.x-k8s.io
machines.cluster.x-k8s.io
...
```

**ğŸ” Ces CRDs sont les "types" que vous utiliserez** dans les modules suivants (Cluster, Machine, etc.)

---

**6.3 - VÃ©rifier les versions installÃ©es**
```bash
clusterctl version
kubectl get deployment -n capi-system capi-controller-manager -o jsonpath='{.spec.template.spec.containers[0].image}'
```

**RÃ©sultat :**
```
clusterctl version: &version.Info{Major:"1", Minor:"11", GitVersion:"v1.11.1"}
registry.k8s.io/cluster-api/cluster-api-controller:v1.11.1
```

---

**6.4 - Explorer les pods de chaque composant**
```bash
kubectl get pods -n capi-system
kubectl get pods -n capd-system
kubectl get pods -n cert-manager
```

---

**6.5 - VÃ©rifier que la socket Docker est accessible**
```bash
# Tester depuis le container kind
docker exec capi-management-control-plane docker ps

# Devrait afficher tous les containers Docker de l'hÃ´te
```

---

## ğŸ“ Points ClÃ©s Ã  Retenir

âœ… **Management Cluster** : Usine Ã  clusters, hÃ©berge les controllers ClusterAPI
âœ… **ClusterAPI Core** : Framework dÃ©claratif (Cluster, Machine CRDs)
âœ… **Docker Provider** : Infrastructure locale rapide (dev/test)
âœ… **cert-manager** : Gestion automatique certificats (dÃ©pendance CAPI)
âœ… **Autres providers** : k0smotron et Helm Addon seront installÃ©s dans les modules suivants

### Architecture RÃ©capitulative

```
Management Cluster (kind)
â”‚
â”œâ”€â”€ ClusterAPI Core (capi-system)
â”‚   â”œâ”€â”€ cluster-controller      â†’ GÃ¨re objets Cluster
â”‚   â”œâ”€â”€ machine-controller      â†’ GÃ¨re objets Machine
â”‚   â””â”€â”€ machinedeployment-controller â†’ GÃ¨re scaling workers
â”‚
â”œâ”€â”€ Bootstrap Provider (capi-kubeadm-bootstrap-system)
â”‚   â””â”€â”€ kubeadm-bootstrap-controller â†’ Configure nodes avec kubeadm
â”‚
â”œâ”€â”€ Control Plane Provider (capi-kubeadm-control-plane-system)
â”‚   â””â”€â”€ kubeadmcontrolplane-controller â†’ GÃ¨re control planes HA
â”‚
â”œâ”€â”€ Infrastructure Provider
â”‚   â””â”€â”€ Docker Provider (capd-system) â†’ CrÃ©e containers
â”‚
â””â”€â”€ Dependencies
    â””â”€â”€ cert-manager â†’ Certificats TLS automatiques
```

---

## â­ï¸ Prochaine Ã‰tape

Management cluster âœ… prÃªt, passez au **Module 01** :

```bash
cd ../01-premier-cluster
cat commands.md
```

**Module 01 :** CrÃ©er votre premier workload cluster avec Docker Provider

---

## ğŸ’¡ Comprendre en Profondeur

> **Note :** Cette section approfondit les concepts techniques. Vous pouvez la sauter et y revenir plus tard.

### Pourquoi kind pour le Management Cluster ?

**kind (Kubernetes IN Docker)** est le choix idÃ©al pour workshops/dev car :

**Avantages :**
- **Setup rapide** : < 1 minute vs 10-15min cloud clusters
- **CoÃ»t zÃ©ro** : Pas de facture AWS/Azure
- **Reproductible** : Configuration identique sur tous les laptops
- **Cleanup facile** : `kind delete cluster` = tout supprimÃ©
- **CI/CD friendly** : Parfait pour pipelines automatisÃ©s

**Limitations (production) :**
- Pas de persistance rÃ©elle (tout en RAM/disk local)
- Pas de HA multi-nodes physiques
- Pas de load balancing cloud
- LimitÃ© par ressources de la machine hÃ´te

**Production Management Cluster :**
```
Environment          Recommendation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dev/Test            â†’ kind
CI/CD               â†’ kind ou EKS/GKE
Staging             â†’ EKS, AKS, GKE (HA)
Production          â†’ EKS, AKS, GKE (HA + backup)
```

---

### Anatomie d'un Cluster kind

**Quand vous exÃ©cutez `kind create cluster` :**

```bash
docker ps
```

Vous voyez :
```
CONTAINER ID   IMAGE                  NAMES
abc123def456   kindest/node:v1.32.0   capi-management-control-plane
```

**Ce container contient un Kubernetes COMPLET :**
- **Control Plane** : API server, etcd, scheduler, controller-manager
- **Kubelet** : Agent node
- **Container Runtime** : containerd
- **CNI** : kindnet (rÃ©seau pod)

**Structure interne :**
```
Container kind (Docker)
â”œâ”€â”€ systemd (init system)
â”œâ”€â”€ containerd (runtime pour pods)
â”œâ”€â”€ kubelet (agent Kubernetes)
â””â”€â”€ Static Pods (dans /etc/kubernetes/manifests/)
    â”œâ”€â”€ kube-apiserver
    â”œâ”€â”€ kube-controller-manager
    â”œâ”€â”€ kube-scheduler
    â””â”€â”€ etcd
```

**Socket Docker MontÃ©e - Architecture Critique :**
```yaml
extraMounts:
  - hostPath: /var/run/docker.sock
    containerPath: /var/run/docker.sock
```

**Pourquoi c'est ESSENTIEL :**
```
Host Machine
â”œâ”€â”€ Docker Daemon (dockerd)
â”‚   â””â”€â”€ Socket: /var/run/docker.sock
â”‚
â”œâ”€â”€ Container kind (management cluster)
â”‚   â”œâ”€â”€ Socket montÃ©e: /var/run/docker.sock â†’ (partagÃ©e avec host)
â”‚   â””â”€â”€ Pod CAPD Controller
â”‚       â””â”€â”€ Utilise la socket pour crÃ©er containers (workload cluster nodes)
â”‚
â””â”€â”€ Containers crÃ©Ã©s par CAPD (workload clusters)
    â”œâ”€â”€ dev-cluster-control-plane-xxx
    â”œâ”€â”€ dev-cluster-worker-xxx
    â””â”€â”€ k0s-demo-cluster-worker-xxx
```

**Flow de crÃ©ation d'un workload cluster :**
```
1. User: kubectl apply -f dev-cluster.yaml
   â†“
2. CAPI Controller (dans kind): DÃ©tecte nouveau Cluster
   â†“
3. CAPD Controller (dans kind): ReÃ§oit DockerMachine Ã  crÃ©er
   â†“
4. CAPD utilise /var/run/docker.sock pour communiquer avec Docker host
   â†“
5. Docker Daemon (host): CrÃ©e containers (nodes du workload cluster)
   â†“
6. Containers crÃ©Ã©s: Apparaissent dans `docker ps` sur le host
```

**Sans le montage de la socket :**
- CAPD ne peut pas crÃ©er de containers
- Les machines restent en "Provisioning" indÃ©finiment
- Erreur: "Cannot connect to Docker daemon"

---

**Port Mapping ExpliquÃ© :**
```yaml
extraPortMappings:
  - containerPort: 30080
    hostPort: 30080
```

**Flow de trafic :**
```
Browser (localhost:30080)
    â†“
Docker Host (port 30080)
    â†“
kind Container (port 30080)
    â†“
Service NodePort (port 30080)
    â†“
Pod Application (port 80)
```

---

### ClusterAPI Init : Que se passe-t-il ?

**Commande :**
```bash
clusterctl init --infrastructure docker
```

**Workflow dÃ©taillÃ© :**

**1. VÃ©rification prÃ©requis (T+0s)**
```
clusterctl vÃ©rifie:
âœ“ kubectl accessible
âœ“ Cluster Kubernetes dÃ©tectÃ© (management cluster)
âœ“ Permissions suffisantes (cluster-admin)
```

**2. Installation cert-manager (T+5s)**
```
Why cert-manager first?
â†’ ClusterAPI utilise webhooks (admission, conversion)
â†’ Webhooks nÃ©cessitent certificats TLS
â†’ cert-manager gÃ©nÃ¨re/renouvelle automatiquement les certs

Installation:
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.18.2/cert-manager.yaml
```

**3. Installation ClusterAPI Core (T+30s)**
```
Namespace: capi-system
Deployment: capi-controller-manager

Controllers inclus:
- Cluster controller (gÃ¨re Cluster CRD)
- Machine controller (gÃ¨re Machine CRD)
- MachineSet controller (gÃ¨re rÃ©plication)
- MachineDeployment controller (gÃ¨re scaling/updates)
- ClusterResourceSet controller (gÃ¨re addons)
```

**4. Installation Bootstrap Provider (T+45s)**
```
Namespace: capi-kubeadm-bootstrap-system
Deployment: capi-kubeadm-bootstrap-controller-manager

RÃ´le:
- GÃ©nÃ¨re cloud-init scripts pour nodes
- Configure kubeadm init/join automatiquement
- Injecte certificats et tokens
```

**5. Installation Control Plane Provider (T+60s)**
```
Namespace: capi-kubeadm-control-plane-system
Deployment: capi-kubeadm-control-plane-controller-manager

RÃ´le:
- GÃ¨re KubeadmControlPlane CRD
- Automatise HA control planes
- Rolling upgrades control planes
- Gestion quorum etcd
```

**6. Installation Docker Provider (T+75s)**
```
Namespace: capd-system
Deployment: capd-controller-manager

RÃ´le:
- CrÃ©e containers Docker pour simuler VMs
- Configure rÃ©seau Docker entre containers
- GÃ¨re DockerCluster, DockerMachine CRDs
```

**VÃ©rification finale :**
```bash
kubectl get pods -A | grep -E "(capi|cert)"
```

**Tous les pods doivent Ãªtre Running (STATUS) et READY (1/1 ou 2/2)**

---


### cert-manager : Pourquoi Indispensable ?

**ClusterAPI utilise des Webhooks Kubernetes :**

**1. Validating Webhooks** : Valident les objets avant crÃ©ation
```yaml
Exemple: CrÃ©ation d'un Cluster
User: kubectl apply -f cluster.yaml
    â†“
API Server: Appelle Validating Webhook
    â†“
ClusterAPI Controller: VÃ©rifie
  âœ“ clusterNetwork.pods.cidrBlocks valide
  âœ“ controlPlaneRef existe
  âœ“ infrastructureRef existe
    â†“
API Server: Accepte ou rejette
```

**2. Mutating Webhooks** : Modifient les objets automatiquement
```yaml
Exemple: CrÃ©ation d'une Machine
User: kubectl apply -f machine.yaml (sans spec.version)
    â†“
API Server: Appelle Mutating Webhook
    â†“
ClusterAPI Controller: Injecte automatiquement
  spec.version: v1.32.8 (depuis Cluster parent)
    â†“
Objet modifiÃ© sauvegardÃ©
```

**3. Conversion Webhooks** : Convertissent entre versions API
```yaml
Manifest ancien: apiVersion: cluster.x-k8s.io/v1alpha3
    â†“
Conversion Webhook: Convertit v1alpha3 â†’ v1beta1
    â†“
Stored version: v1beta1
```

**Webhooks NÃ‰CESSITENT TLS :**
- API server communique avec webhooks via HTTPS
- Sans certificats valides = erreur webhook call failed

**cert-manager automatise :**
```
1. GÃ©nÃ¨re CA (Certificate Authority) privÃ©e
2. CrÃ©e certificats pour chaque webhook
3. Injecte caBundle dans webhook configurations
4. Renouvelle automatiquement avant expiration (90 jours)
```

**Sans cert-manager :**
```bash
# GÃ©nÃ©ration manuelle (complexe, erreur-prone)
openssl genrsa -out ca.key 2048
openssl req -x509 -new -nodes -key ca.key -subj "/CN=webhook-ca" -days 3650 -out ca.crt
openssl genrsa -out webhook.key 2048
openssl req -new -key webhook.key -subj "/CN=webhook.capi-system.svc" -out webhook.csr
openssl x509 -req -in webhook.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out webhook.crt -days 365
kubectl create secret tls webhook-cert --cert=webhook.crt --key=webhook.key -n capi-system
# ... rÃ©pÃ©ter pour chaque webhook
# ... renouvellement manuel tous les ans
```

**Avec cert-manager :**
```yaml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: capi-webhook-cert
spec:
  secretName: capi-webhook-cert
  duration: 8760h  # 1 an
  renewBefore: 720h  # Renouveler 30 jours avant
  issuerRef:
    name: capi-selfsigned-issuer
    kind: Issuer
  dnsNames:
    - webhook.capi-system.svc
    - webhook.capi-system.svc.cluster.local
```

**RÃ©sultat :** AutomatisÃ©, sÃ©curisÃ©, sans intervention humaine.

---

## ğŸ”§ DÃ©pannage

### Socket Docker Non MontÃ©e

**SymptÃ´me :** Dans les modules suivants, les workload clusters ne se crÃ©ent pas (machines restent en Provisioning)

**Cause :** Socket Docker (`/var/run/docker.sock`) non montÃ©e dans le cluster kind

**Diagnostic :**
```bash
# VÃ©rifier si la socket est accessible depuis le cluster kind
docker exec capi-management-control-plane ls -la /var/run/docker.sock

# VÃ©rifier que les pods CAPD peuvent communiquer avec Docker
kubectl logs -n capd-system deployment/capd-controller-manager | grep -i "docker"
```

**Solution :**
```bash
# RecrÃ©er le cluster avec la socket Docker montÃ©e
kind delete cluster --name capi-management

cat > management-cluster-config.yaml << 'EOF'
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: capi-management
nodes:
  - role: control-plane
    extraMounts:
      - hostPath: /var/run/docker.sock
        containerPath: /var/run/docker.sock
    extraPortMappings:
      - containerPort: 30080
        hostPort: 30080
        protocol: TCP
    kubeadmConfigPatches:
      - |
        kind: InitConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "ingress-ready=true"
            authorization-mode: "Webhook"
EOF

kind create cluster --config management-cluster-config.yaml
clusterctl init --infrastructure docker
```

**VÃ©rification :**
```bash
# Tester l'accÃ¨s Docker depuis le cluster
kubectl run -it --rm debug --image=docker:latest --restart=Never -- docker ps

# Vous devriez voir les containers Docker de l'hÃ´te
```

---

### ClusterAPI Init Ã‰choue

**SymptÃ´me :** `clusterctl init` timeout ou erreur

**Diagnostic :**
```bash
# VÃ©rifier permissions kubectl
kubectl auth can-i create namespaces
kubectl auth can-i create crds

# VÃ©rifier connexion internet (tÃ©lÃ©chargement manifests)
curl -I https://github.com

# Logs de clusterctl
clusterctl init --infrastructure docker -v 5  # Verbosity max
```

**Solutions :**
```bash
# Permissions insuffisantes
kubectl config view --minify  # VÃ©rifier le contexte actuel
# Utiliser admin kubeconfig ou accorder cluster-admin

# Network issues
# Utiliser --config pour spÃ©cifier mirrors locaux
clusterctl init --infrastructure docker --config clusterctl.yaml

# Retry avec cleanup
clusterctl delete --infrastructure docker --include-crd
clusterctl init --infrastructure docker
```

---


## ğŸ“Š Validation ComplÃ¨te

### Checklist Finale

**Management Cluster :**
- [ ] Cluster kind "capi-management" existe
- [ ] Contexte kubectl configurÃ©: `kind-capi-management`
- [ ] Node status: Ready

**ClusterAPI Core :**
- [ ] Namespace `capi-system` existe
- [ ] Deployment `capi-controller-manager` : Running
- [ ] CRDs installÃ©es : `kubectl get crd | grep cluster.x-k8s.io` montre 20+ CRDs

**Providers :**
- [ ] Docker Provider (capd-system) : Running

**Dependencies :**
- [ ] cert-manager pods : Running (3/3)

**Commande Unique de Validation :**
```bash
cat > full-check.sh << 'EOF'
#!/bin/bash
echo "ğŸ” Validation ComplÃ¨te Management Cluster"
echo "=========================================="

# Check cluster exists
if kind get clusters 2>/dev/null | grep -q "capi-management"; then
  echo "âœ… Cluster kind existe"
else
  echo "âŒ Cluster kind manquant"
  exit 1
fi

# Check all namespaces
for ns in capi-system capd-system cert-manager; do
  if kubectl get namespace $ns &>/dev/null; then
    echo "âœ… Namespace $ns existe"
  else
    echo "âŒ Namespace $ns manquant"
    exit 1
  fi
done

# Check all deployments running
DEPLOYMENTS=(
  "capi-system/capi-controller-manager"
  "capd-system/capd-controller-manager"
  "cert-manager/cert-manager"
)

for deploy in "${DEPLOYMENTS[@]}"; do
  ns=$(echo $deploy | cut -d'/' -f1)
  name=$(echo $deploy | cut -d'/' -f2)
  if kubectl get deployment -n $ns $name &>/dev/null; then
    ready=$(kubectl get deployment -n $ns $name -o jsonpath='{.status.readyReplicas}')
    desired=$(kubectl get deployment -n $ns $name -o jsonpath='{.spec.replicas}')
    if [ "$ready" == "$desired" ]; then
      echo "âœ… Deployment $deploy : $ready/$desired ready"
    else
      echo "âŒ Deployment $deploy : $ready/$desired ready"
      exit 1
    fi
  else
    echo "âŒ Deployment $deploy manquant"
    exit 1
  fi
done

echo "=========================================="
echo "ğŸ‰ Validation complÃ¨te rÃ©ussie!"
echo "ğŸš€ Management cluster opÃ©rationnel"
EOF

chmod +x full-check.sh
./full-check.sh
```

---

## ğŸ“ Ce Que Vous Avez Appris

âœ… CrÃ©er un cluster kind configurÃ© pour ClusterAPI
âœ… Initialiser ClusterAPI avec le Docker Provider
âœ… Comprendre l'architecture Management vs Workload
âœ… Valider l'installation complÃ¨te des composants

**Architecture Finale :**
```
Management Cluster (kind) âœ…
â”œâ”€â”€ ClusterAPI v1.11.1 âœ…
â”œâ”€â”€ Docker Provider âœ…
â””â”€â”€ cert-manager v1.18.2 âœ…

PrÃªt Ã  crÃ©er â†’ Workload Clusters!
(k0smotron et Helm Addon seront installÃ©s dans les modules suivants)
```

---

**Module 00-setup complÃ©tÃ© ! ğŸ‰**
**Temps Ã©coulÃ© :** 15 minutes
**Prochaine Ã©tape :** Module 01 - Premier Cluster avec Docker Provider
